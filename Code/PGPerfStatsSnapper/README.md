# PostgreSQL Performance Stats Snapper

While doing load testing on RDS/Aurora PostgreSQL for proof of concept (POC) or for testing the impact of any configuration or code change, its important to collect all the database performance related metrics periodically at a certain interval for post analysis.
RDS Enhanced Monitoring and RDS Performance Insights collect a lot of database performance metrics and provide dashboards for viewing the historical data. There are a lot of other database performance statistics and metrics, which can be collected to assist with deep dive analysis of performance problems post the load testing.

The snapper script provided here enables periodic collection (snapping) of PostgreSQL performance related statistics and metrics. The config file used by the script can be customized to add and remove database dictionary views and queries to be snapped as required.
The snapper script collects and stores the PostgreSQL database metrics in separate OS level files to have minimal impact on the database. These files can be loaded into another PostgreSQL instance by the loader script for post analysis.

## Prerequisites

1. Add **pg_stat_statements** to [shared_preload_libraries](https://www.postgresql.org/docs/11/runtime-config-client.html) DB parameter and create required extensions by running the following in the PostgreSQL instance. 
 Set **track_activity_query_size** parameter to the max value 102400 to capture the full text of very long SQL statements.

   **Note:**  ```aurora_stat_utils``` extension is valid only for Aurora PostgreSQL.
```bash
psql --host=<PostgreSQL Instance EndPoint> --port=<Port> --username=<Master UserName> --dbname=<Database Name where Application objects are stored>

postgres=> create extension pg_stat_statements;
postgres=> create extension aurora_stat_utils;
```
2. Discard all statistics gathered by pg_stat_statements before running load test by running the following:
```bash
psql --host=<PostgreSQL Instance EndPoint> --port=<Port> --username=<Master UserName> --dbname=<Database Name where Application objects are stored>

postgres=> SELECT pg_stat_statements_reset();
```

3. Store database master credential in [AWS secret manager](https://docs.aws.amazon.com/secretsmanager/latest/userguide/manage_create-basic-secret.html) and note down the secret ARN. This needs to be provided as a parameter to the snapper script to retrieve database credential for logging into the PostgreSQL instance.

4. Create an [IAM role for EC2](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html#create-iam-role) Service with an inline policy similar to the following. This enables the EC2 instance (which is used to schedule the snapper script) to retrieve AWS Secrets Manager stored secrets and to put/pre-sign files in AWS S3.
```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "VisualEditor0",
            "Effect": "Allow",
            "Action": "s3:*",
            "Resource": "*"
        },
        {
        "Effect": "Allow",
        "Action": "secretsmanager:GetSecretValue",
        "Resource": "*"
        }
    ]
}
```
5. [Create a S3 bucket](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-bucket.html) for uploading and sharing the output generated by the snapper script. You can create the S3 bucket with all the default options.

## Setup

1. [Launch an EC2 instance](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/launching-instance.html) where the snapper script will be scheduled.
   1. Use *"Amazon Linux AMI 2018.03.0 (HVM), SSD Volume Type"* AMI.
   1. Use r4.8xlarge instance type.
   1. Use same VPC and subnet/AZ as the RDS/Aurora PostgreSQL Instance
   1. Add the IAM role to the EC2 instance which was created in the prerequisite step.
   1. Allocate 30 GB of General Purpose SSD (gp2) storage to the Root volume.
   1. Add the EC2 instance to the same security group as the RDS/Aurora instance or add it to a new security group (and later add the new security group to inbound rule of the RDS/Aurora security group).
   1. Make sure to download and save the key pair.

2. Add appropriate [inbound rules](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/authorizing-access-to-an-instance.html) to the security group used the previous step to allow your workstation to be able to SSH to the EC2 instance.

3. Optional. If you created a new security group while launching EC2, make sure to add that security group to inbound rule of the RDS/Aurora security group for the EC2 instance to be able to communicate with the PostgreSQL instance.

4. Install PostgreSQL client on the EC2 instance
```bash
sudo yum -y update
sudo yum -y group install "Development Tools"
sudo yum -y install readline-devel
sudo yum -y install openssl-devel
mkdir ~/postgresql
cd ~/postgresql
curl https://ftp.postgresql.org/pub/source/v11.3/postgresql-11.3.tar.gz -o postgresql-11.3.tar.gz
tar -xvf postgresql-11.3.tar.gz
cd postgresql-11.3
sudo ./configure --with-openssl
sudo make -C src/bin install
sudo make -C src/include install
sudo make -C src/interfaces install
sudo make -C doc install
sudo /sbin/ldconfig /usr/local/pgsql/lib
```

5. Install Python dependencies
```bash
sudo su -
PATH=/usr/local/pgsql/bin:$PATH
export PATH
pip install boto3
pip install PyGreSQL
exit
```
6. Download the snapper Python script along with the config file from [github](https://github.com/aws-samples/aurora-and-database-migration-labs/tree/master/Code/PGPerfStatsSnapper) and stage it in a directory. We will be using ```/home/ec2-user/scripts``` as the staging directory in the below steps.
```bash
mkdir -p /home/ec2-user/scripts
cd /home/ec2-user/scripts
curl -L https://raw.githubusercontent.com/aws-samples/aurora-and-database-migration-labs/master/Code/PGPerfStatsSnapper/pg_perf_stat_snapper.py -o pg_perf_stat_snapper.py
curl -L https://raw.githubusercontent.com/aws-samples/aurora-and-database-migration-labs/master/Code/PGPerfStatsSnapper/config_pg_perf_stat_snapper.json -o config_pg_perf_stat_snapper.json
chmod 755 pg_perf_stat_snapper.py
```

Script Usage:

```bash
[ec2-user@ip-172-31-1-240 scripts]$ /home/ec2-user/scripts/pg_perf_stat_snapper.py -h
usage: pg_perf_stat_snapper.py [-h] -e ENDPOINT -P PORT -d DBNAME -u USER -s
                               SECRETARN -m MODE [-o OUTPUTDIR] -r REGION

Snap PostgreSQL performance statistics and exit

optional arguments:
  -h, --help            show this help message and exit
  -e ENDPOINT, --endpoint ENDPOINT
                        PostgreSQL Instance Endpoint (default: None)
  -P PORT, --port PORT  Port (default: None)
  -d DBNAME, --dbname DBNAME
                        Database Name where Application objects are stored
                        (default: None)
  -u USER, --user USER  Database UserName (default: None)
  -s SECRETARN, --SecretARN SECRETARN
                        AWS Secrets Manager stored Secret ARN (default: None)
  -m MODE, --mode MODE  Mode in which the script will run: Specify either snap
                        or package (default: None)
  -o OUTPUTDIR, --outputdir OUTPUTDIR
                        Output Directory (default:
                        /home/ec2-user/scripts/output)
  -r REGION, --region REGION
                        AWS region (default: None)
```

7. Schedule the script in crontab to run every 1 minute. Here the <output directory> is optional and if not provided all the output will be stored under "output" subdirectory where the script is staged.
```bash
*/1 * * * * /home/ec2-user/scripts/pg_perf_stat_snapper.py -e <PostgreSQL Instance EndPoint> -P <Port> -d <Database Name where Application objects are stored> -u <Master UserName> -s <AWS Secretes Manager ARN> -m snap [-o <output directory>] -r <AWS Region>
```
## Load Test

1. Perform Load test after the snapper script is scheduled in crontab.

2. Once load test is complete, comment out crontab to disable the snapper scheduled runs.

## Packaging the Output

1. Package the snapper output by running the following:
```bash
/home/ec2-user/scripts/pg_perf_stat_snapper.py -e <PostgreSQL Instance EndPoint> -P <Port> -d <Database Name where Application objects are stored> -u <Master UserName> -s <AWS Secretes Manager ARN> -m package [-o <output directory>] -r <AWS Region>
```
2. Zip the output directory, upload to the S3 bucket created in the prerequisite section and create a pre-signed URL of the zip file. In the example below ```s3://pg-snapper-output/``` is the bucket used for uploading the zip file.
```bash
cd /home/ec2-user/scripts
zip -r pg-snapper-output output
aws s3 cp pg-snapper-output.zip s3://pg-snapper-output/
aws s3 presign s3://pg-snapper-output/pg-snapper-output.zip --expires-in 604800
```
3. Share the S3 URL for loading the output and do further analysis.
